{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "import numpy\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas\n",
    "import datetime\n",
    "from functools import partial, reduce\n",
    "\n",
    "import sys\n",
    "sys.path.append('../libs')\n",
    "\n",
    "import data_pipeline\n",
    "import conv_model\n",
    "import initialize\n",
    "import prepare_data\n",
    "import flacdb\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 23 11:13:21 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX TIT...  On   | 00000000:89:00.0 Off |                  N/A |\n",
      "| 22%   38C    P8    16W / 250W |      0MiB / 12212MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pandas.read_csv('/scr-ssd/mimic/metadata_matched.csv')\n",
    "subject_ids = metadata['subject_id'].unique()\n",
    "numpy.random.shuffle(subject_ids)\n",
    "i = round(0.2*len(subject_ids))\n",
    "# with open('../test_subject_ids.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(subject_ids[:i].astype('str')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 years,  188592 record segments\n",
      "CPU times: user 36 s, sys: 7.65 s, total: 43.7 s\n",
      "Wall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "H = initialize.load_hypes()\n",
    "sig_data, metadata, partition = initialize.load_data(H)\n",
    "initialize.describe_data_size(H, sig_data, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['train', 'validation']:\n",
    "    data[k] = data_pipeline.build(H, data[k], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = initialize.sample_data(H_, sig_data, 'train')\n",
    "tensors = initialize.dataframe_to_tensors(H_, dataframe)\n",
    "dataset = data_pipeline.build(H_, tensors, is_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot\n",
    "from functools import partial\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "\n",
    "S = H['output_sigs'] + H['input_sigs_train']\n",
    "        \n",
    "def plot_batch_example(H, x, y, i):\n",
    "    pyplot.subplots_adjust(left=0.03, wspace=0, hspace=0)\n",
    "    lines = {'sigs': {}, 'label': {}}\n",
    "    axes = {}\n",
    "    for j, s in enumerate(S):\n",
    "        axes[s] = pyplot.subplot(len(S), 1, j + 1)\n",
    "        line = axes[s].plot(x[i][:, j])[0]\n",
    "        lines['sigs'][s] = line\n",
    "        axes[s].set_ylabel(s)\n",
    "        axes[s].yaxis.tick_right()\n",
    "        if s in ['ABP', 'ART', 'CVP', 'PAP', 'ICP']:\n",
    "            line.set_color('red')\n",
    "        elif s in ['I', 'II', 'III', 'V', 'AVR', 'AVF', 'AVL', 'MCL', 'MCL1']:\n",
    "            line.set_color('darkblue')\n",
    "        elif s in ['RESP']:\n",
    "            line.set_color('darkgreen')\n",
    "        elif s in ['PLETH']:\n",
    "            line.set_color('darkred')\n",
    "    \n",
    "    plot_y = lambda j: axes['ABP'].plot([0, H['window_size']], [y[i][j]] * 2, '--k')[0]\n",
    "    lines['label'] = {'sys': plot_y(0), 'dia': plot_y(1)}\n",
    "    \n",
    "    return lines, axes\n",
    "\n",
    "def update(i):\n",
    "    for j, s in enumerate(S):\n",
    "        lines['sigs'][s].set_ydata(x[i][:, j])\n",
    "        low, high = x[i][:, j].min(), x[i][:, j].max()\n",
    "        dx = max(0.01, high - low)\n",
    "        axes[s].set_ylim(bottom = low - 0.1 * dx, top = high + 0.1 * dx)\n",
    "    lines['label']['sys'].set_ydata([y[i][0]] * 2)\n",
    "    lines['label']['dia'].set_ydata([y[i][1]] * 2)\n",
    "    fig.axes[0].set_title('Systolic: %.1f, Diastolic: %.1f' % tuple(y[i]))\n",
    "    fig.canvas.draw()\n",
    "    pyplot.show()\n",
    "\n",
    "H_ = {\n",
    "    **H, \n",
    "    'input_sigs_train': ['ABP'] + H['input_sigs_train'], \n",
    "    'input_sigs_validation': ['ABP'] + H['input_sigs_validation'], \n",
    "    'epochs': 1, \n",
    "    'steps_per_epoch': 1,\n",
    "    'batch_buffer_size': 1,\n",
    "    'windows_per_chunk': 5,\n",
    "}\n",
    "\n",
    "dataframe = initialize.sample_data(H_, sig_data, is_validation=True)\n",
    "tensors = initialize.dataframe_to_tensors(H_, dataframe)\n",
    "dataset = data_pipeline.build(H_, tensors, is_validation=True)\n",
    "x_tf, y_tf = next(iter(dataset))\n",
    "x, y = x_tf.numpy(), y_tf.numpy()\n",
    "\n",
    "fig = pyplot.figure(figsize=[8, 6])\n",
    "lines, axes = plot_batch_example(H_, x, y, 0)\n",
    "\n",
    "interact(update, i=IntSlider(min=0, max=H_['batch_size']-1, value=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "N = 10**4\n",
    "\n",
    "H_ = {\n",
    "    **H, \n",
    "    'epochs': 1,\n",
    "    'steps_per_epoch': N / H['batch_size'] / 10 * 2,\n",
    "    'batch_buffer_size': 1,\n",
    "    'windows_per_chunk': 10,\n",
    "    'filter_data': True\n",
    "}\n",
    "\n",
    "dataframe = initialize.sample_data(H_, sig_data)\n",
    "print(dataframe.shape)\n",
    "I = numpy.random.permutation(dataframe.shape[0] // H_['windows_per_chunk'])[:N // H_['windows_per_chunk']]\n",
    "I = [i*H_['windows_per_chunk'] + j for i in I for j in range(H_['windows_per_chunk'])]\n",
    "tensors = initialize.dataframe_to_tensors(H_, dataframe.iloc[I])\n",
    "dataset = data_pipeline.build(H_, tensors)\n",
    "n = sum(i[0].shape[0] for i in dataset)\n",
    "round(n / N * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "N = 10**4\n",
    "\n",
    "H_ = {\n",
    "    **H, \n",
    "    'epochs': 1,\n",
    "    'steps_per_epoch': N / H['batch_size'] / 10 * 2,\n",
    "    'batch_buffer_size': 1,\n",
    "    'windows_per_chunk': 10,\n",
    "    'filter_data': True\n",
    "}\n",
    "\n",
    "dataframe = initialize.sample_data(H_, sig_data)\n",
    "print(dataframe.shape)\n",
    "I = numpy.random.permutation(dataframe.shape[0] // H_['windows_per_chunk'])[:N // H_['windows_per_chunk']]\n",
    "I = [i*H_['windows_per_chunk'] + j for i in I for j in range(H_['windows_per_chunk'])]\n",
    "tensors = initialize.dataframe_to_tensors(H_, dataframe.iloc[I])\n",
    "dataset = data_pipeline.build(H_, tensors)\n",
    "n = sum(i[0].shape[0] for i in dataset)\n",
    "round(n / N * 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dataset = {\n",
    "    'train': pipeline.build(H, dataframes['train']),\n",
    "    'validation': pipeline.build(H, dataframes['validation']),\n",
    "}\n",
    "\n",
    "model = conv_model.build(H)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "logdir = os.path.join('/scr-ssd/tflogs', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, embeddings_freq=5)\n",
    "\n",
    "model.fit(\n",
    "    dataset['train'],\n",
    "    validation_data = dataset['validation'],\n",
    "    epochs = H['epochs'],\n",
    "    steps_per_epoch = H['steps_per_epoch'],\n",
    "    validation_steps = H['steps_per_epoch'],\n",
    "    callbacks = [tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simple_fc_model(H):\n",
    "    inputs = tf.keras.layers.Input(shape=(H['window_size'], len(H['input_sigs'])))\n",
    "    z = tf.keras.layers.Flatten()(inputs)\n",
    "    for i in range(3):\n",
    "        z = tf.keras.layers.Dense(H['dense_units'], activation=H['activation'])(z)\n",
    "    final_layer = tf.keras.layers.Dense(2)\n",
    "    outputs = final_layer(z)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    final_layer.set_weights([\n",
    "        final_layer.get_weights()[0],\n",
    "        tf.keras.backend.constant([120, 60], dtype='float32')\n",
    "    ])\n",
    "\n",
    "    optimizer = getattr(tf.keras.optimizers, H['optimizer']['name'].title())\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = optimizer(**H['optimizer']['args']),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mean_absolute_error']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simple_conv_model(H):\n",
    "    inputs = z = tf.keras.layers.Input(shape=(H['window_size'], len(H['input_sigs'])))\n",
    "\n",
    "    for i in range(3):\n",
    "        layer = tf.keras.layers.Conv1D(\n",
    "            filters=128, \n",
    "            padding='same', \n",
    "            strides=4,\n",
    "            kernel_size=32,\n",
    "            activation='relu'\n",
    "        )\n",
    "        z = layer(z)\n",
    "\n",
    "    z = tf.keras.layers.Flatten()(z)\n",
    "    z = tf.keras.layers.Dense(128, activation='relu')(z)\n",
    "    final_layer = tf.keras.layers.Dense(2)\n",
    "    outputs = final_layer(z)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    final_layer.set_weights([\n",
    "        final_layer.get_weights()[0],\n",
    "        tf.keras.backend.constant([120, 60], dtype='float32')\n",
    "    ])\n",
    "\n",
    "    optimizer = getattr(tf.keras.optimizers, H['optimizer']['name'].title())\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = optimizer(**H['optimizer']['args']),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mean_absolute_error']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv_model.build(H)\n",
    "data = sample_data(H)\n",
    "x, y = next(iter(data['train']))\n",
    "model.predict_on_batch(x).numpy().mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfit Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = sample_data(H)\n",
    "x, y = next(iter(data['train']))\n",
    "data['train'] = tf.data.Dataset.from_tensors((x, y)).repeat(5000)\n",
    "model.fit(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = simple_conv_model(H)\n",
    "for i in range(3):\n",
    "    model.fit(data['train'].take(100).map(lambda x, y: (x*0, y)))\n",
    "    model.evaluate(data['validation'].take(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = simple_conv_model(H)\n",
    "for i in range(3):\n",
    "    data = sample_data(H)\n",
    "    model.fit(data['train'])\n",
    "    model.evaluate(data['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --host '0.0.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op GatherV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op GatherV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op GatherV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op GatherV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op GatherV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Range in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Pack in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op GatherV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op BroadcastTo in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Pack in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FilterDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousRandomSeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ShuffleDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelInterleaveDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FilterDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "H = initialize.load_hypes()\n",
    "\n",
    "with open('/scr-ssd/mimic/initial_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "for k in ['train', 'validation']:\n",
    "    data[k] = data_pipeline.build(H, data[k], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.937294\n",
      "CPU times: user 18min 14s, sys: 32 s, total: 18min 46s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n = 10**5\n",
    "n //= H['batch_size']\n",
    "y_ = tf.constant(0, dtype='float32')\n",
    "for x, y in data['train'].take(n):\n",
    "    y_ += tf.reduce_mean(y, axis=0)\n",
    "y_ /= n\n",
    "print(y_.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_keras_scratch_graph_18972 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_18977 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_18982 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_18987 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_18992 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_19061 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "3125/3125 [==============================] - 550s 176ms/step - loss: 18.1367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.136708411254883"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.keras.layers.Input(shape=(H['window_size'], len(H['input_sigs_train'])))\n",
    "z = x[:1, 0, 0]\n",
    "z *= tf.constant(0, dtype='float32')\n",
    "z += y_\n",
    "\n",
    "const_model = tf.keras.models.Model(inputs=x, outputs=z)\n",
    "const_model.compile(loss='mean_absolute_error')\n",
    "const_model.evaluate(data['validation'].take(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 5000\n",
    "y_ = tf.constant([0, 0], dtype='float32')\n",
    "for x, y in data['train'].take(n):\n",
    "    y_ += tf.reduce_mean(y, axis=0)\n",
    "y_ /= n\n",
    "print(y_.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_util.calculate_training_speed(H, 5000, seconds_to_train=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame([\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'flac',   'Windows': 10,   'Speed (days/hr)': 14},\n",
    "    {'CPU count': 16, 'GPU count': 6, 'Format': 'flac',   'Windows': 10,   'Speed (days/hr)': 22},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'flac',   'Windows': 50,   'Speed (days/hr)': 65},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'flac',   'Windows': 1000, 'Speed (days/hr)': 107},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'flac',   'Windows': 1000, 'Speed (days/hr)': 182},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'zlib',   'Windows': 10,   'Speed (days/hr)': 24},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'zlib',   'Windows': 50,   'Speed (days/hr)': 93},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'zlib',   'Windows': 100,  'Speed (days/hr)': 112},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'serial', 'Windows': 10,   'Speed (days/hr)': 68},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'serial', 'Windows': 50,   'Speed (days/hr)': 178},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'serial', 'Windows': 100,  'Speed (days/hr)': 210},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'serial', 'Windows': 1000, 'Speed (days/hr)': 311},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'chunks', 'Windows': 10,   'Speed (days/hr)': 248},\n",
    "    {'CPU count': 16, 'GPU count': 3, 'Format': 'memory', 'Windows': -1,   'Speed (days/hr)': 345},\n",
    "    {'CPU count': 16, 'GPU count': 6, 'Format': 'memory', 'Windows': -1,   'Speed (days/hr)': 459},\n",
    "])\n",
    "df.sort_values('Speed (days/hr)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Data in Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_batches = 1000\n",
    "\n",
    "x = tf.random.uniform(\n",
    "    shape = [n_batches, H['batch_size'], H['window_size'], len(H['input_sigs'])],\n",
    "    minval = -1,\n",
    "    maxval = 1,\n",
    ")\n",
    "y = tf.random.uniform(shape=[n_batches, H['batch_size'], 2], minval=40, maxval=200)\n",
    "dummy_data = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "model = conv_model.build(H)\n",
    "\n",
    "%time model.fit(dummy_data, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
