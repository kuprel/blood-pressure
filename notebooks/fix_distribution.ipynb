{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "import numpy\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas\n",
    "import datetime\n",
    "from functools import partial, reduce\n",
    "\n",
    "import sys\n",
    "sys.path.append('../libs')\n",
    "\n",
    "import data_pipeline\n",
    "import conv_model\n",
    "import initialize\n",
    "import prepare_data\n",
    "import flacdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = initialize.load_hypes()\n",
    "H = {\n",
    "    **H, \n",
    "    'epochs': 16, \n",
    "    'window_size': 512,\n",
    "    'steps_per_epoch': 32,\n",
    "    'validation_steps': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_initial_data():\n",
    "    metadata = pandas.read_hdf('/scr-ssd/mimic/metadata.hdf')\n",
    "    metadata = metadata[metadata['sig_len'] > prepare_data.CHUNK_SIZE]\n",
    "    chunk_counts = metadata['sig_len'].apply(prepare_data.get_chunk_count)\n",
    "    metadata.at[:, 'chunk_count'] = chunk_counts.astype('uint8')\n",
    "#     index = (metadata.index & prepare_data.get_downloaded()).sort_values()\n",
    "#     metadata = metadata.reindex(index)\n",
    "    missing = metadata['subject_id'] == -1\n",
    "    fake_ids = -metadata.loc[missing].index.get_level_values(0)\n",
    "    metadata.at[missing, 'subject_id'] = fake_ids\n",
    "    subject_ids = metadata['subject_id']\n",
    "    metadata = metadata.reset_index()\n",
    "    metadata.set_index(['subject_id', 'rec_id', 'segment'], inplace=True, verify_integrity=True)\n",
    "    metadata.sort_index(inplace=True)\n",
    "    index_names = ['rec_id', 'segment', 'sig_name']\n",
    "    columns = index_names + ['sig_index', 'baseline', 'adc_gain']\n",
    "    sig_data = pandas.read_hdf('/scr-ssd/mimic/sig_data.hdf', columns=columns)\n",
    "    sig_data.drop_duplicates(index_names, inplace=True)\n",
    "    sig_data = sig_data.astype({'sig_name': str})\n",
    "    dtypes = sig_data.dtypes\n",
    "    sig_data.set_index(index_names, inplace=True)\n",
    "    sig_data.at[:, 'sig_index'] += 1\n",
    "    sig_data.at[:, 'subject_id'] = subject_ids\n",
    "    sig_data.reset_index(inplace=True)\n",
    "    sig_data.set_index(['subject_id'] + index_names, inplace=True, verify_integrity=True)\n",
    "    sig_data = sig_data.unstack(fill_value=0)\n",
    "    sig_data = sig_data.astype({(k, s): dtypes[k] for k, s in sig_data})\n",
    "    sig_data = sig_data[sig_data['sig_index']['ABP'] > 0]\n",
    "    index = (metadata.index & sig_data.index).sort_values()\n",
    "    metadata = metadata.reindex(index)\n",
    "    sig_data = sig_data.reindex(index)\n",
    "    return sig_data, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 15.3 s, total: 1min 18s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sig_data, metadata = load_initial_data()\n",
    "path = '/scr1/mimic/initial_data/'\n",
    "sig_data.to_pickle(path + 'sig_data.pkl')\n",
    "metadata.to_pickle(path + 'metadata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diagnosis(codes, metadata):\n",
    "    diagnosis = pandas.read_csv(initialize.clinic_file('diagnoses_icd'))\n",
    "    new_names = {i.upper(): i for i in ['subject_id', 'hadm_id']}\n",
    "    new_names['ICD9_CODE'] = 'code'\n",
    "    diagnosis = diagnosis[new_names].rename(columns=new_names)\n",
    "    diagnosis.loc[~diagnosis['code'].isin(codes), 'code'] = 'other'\n",
    "    diagnosis.drop_duplicates(inplace=True)\n",
    "    diagnosis.set_index(['subject_id', 'hadm_id', 'code'], inplace=True)\n",
    "    diagnosis.sort_index(inplace=True)\n",
    "    diagnosis.at[:, 'present'] = True\n",
    "    diagnosis = diagnosis.unstack(fill_value=False)['present'].astype('bool')\n",
    "    \n",
    "    matched_data = metadata.reset_index()\n",
    "    matched_data = matched_data[matched_data['subject_id'] > 0]    \n",
    "    matched_data.drop_duplicates(['subject_id', 'hadm_id'], inplace=True)\n",
    "    matched_data.set_index(['subject_id', 'hadm_id'], inplace=True)\n",
    "    matched_data.sort_index(inplace=True)\n",
    "        \n",
    "    diagnosis = diagnosis.reindex(matched_data.index)\n",
    "    \n",
    "    for i in ['gender', 'ethnicity']:\n",
    "        values = matched_data[i]\n",
    "        for j in values.dtype.categories:\n",
    "            diagnosis.at[values.notna(), i + '_' + j] = values == j\n",
    "    \n",
    "    died = matched_data['death_time'].notna()\n",
    "    diagnosis.at[(slice(None), slice(0, None)), 'died'] = died\n",
    "\n",
    "    index = metadata.reset_index()[diagnosis.index.names]\n",
    "    index = pandas.MultiIndex.from_frame(index)\n",
    "    diagnosis = diagnosis.reindex(index).reset_index()\n",
    "    frames = [diagnosis, metadata.reset_index()[['rec_id', 'segment']]]\n",
    "    diagnosis = pandas.concat(frames, sort=False, axis=1)\n",
    "\n",
    "    diagnosis = diagnosis.set_index(['subject_id', 'rec_id', 'segment'])\n",
    "    diagnosis = diagnosis.sort_index()\n",
    "\n",
    "    is_negative_always = ~diagnosis.drop(columns='hadm_id').any(level=0)\n",
    "    is_diagnosed_always = (diagnosis['hadm_id'] > 0).all(level=0)\n",
    "\n",
    "    is_negative = is_negative_always[is_diagnosed_always]\n",
    "    diagnosis = diagnosis.drop(columns='hadm_id')\n",
    "    bool_to_int = {True: 1, False: 0, numpy.nan: 0}\n",
    "    diagnosis = diagnosis.replace(bool_to_int)\n",
    "    diagnosis.loc[is_negative.index] -= is_negative.replace(bool_to_int)\n",
    "    diagnosis = diagnosis.reset_index()\n",
    "    diagnosis = diagnosis.set_index(metadata.index.names)\n",
    "    diagnosis = diagnosis.reindex(metadata.index)\n",
    "        \n",
    "    return diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/scr1/mimic/initial_data/'\n",
    "metadata = pandas.read_pickle(path + 'metadata.pkl')\n",
    "sig_data = pandas.read_pickle(path + 'sig_data.pkl')\n",
    "sig_data = sig_data.loc[:, (slice(None), H['input_sigs'] + H['output_sigs'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.76 s, sys: 708 ms, total: 7.47 s\n",
      "Wall time: 7.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "diagnosis = load_diagnosis(H['icd_codes'], metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(H, metadata, sig_data, diagnosis):\n",
    "    assert((metadata.index == sig_data.index).all())\n",
    "    assert((metadata.index == diagnosis.index).all())\n",
    "    \n",
    "    tensors = {}\n",
    "    \n",
    "    get_rec_ids = lambda i: [i.index.get_level_values(1).unique()]\n",
    "    rec_ids = metadata.groupby(level=0).apply(get_rec_ids)\n",
    "    tensors['rec_ids'] = tf.ragged.constant(rec_ids, dtype='int32')\n",
    "    \n",
    "    row_lengths = metadata.groupby(level=[0, 1]).apply(len)\n",
    "    row_lengths = row_lengths.groupby(level=0).apply(lambda i: i.values)\n",
    "    row_lengths = [\n",
    "        row_lengths.apply(len).values,\n",
    "        numpy.concatenate(row_lengths.values)\n",
    "    ]\n",
    "    \n",
    "    def to_nested_ragged_tensor(df, k):\n",
    "        values = df.values.astype(initialize.TENSOR_DTYPES[k])\n",
    "        tensor = tf.RaggedTensor.from_nested_row_lengths(values, row_lengths)\n",
    "        return tensor\n",
    "    \n",
    "    \n",
    "    reducers = {\n",
    "        'height': lambda i: i.max(),\n",
    "        'weight': lambda i: i.mean(),\n",
    "        'age': lambda i: i.max(),\n",
    "        'diagnosis': lambda i: i.iloc[0]\n",
    "    }\n",
    "    \n",
    "    def to_ragged_tensor(df, k):\n",
    "        values = df.groupby(level=[0, 1]).apply(reducers[k])\n",
    "        to_values = lambda i: [i.values.astype(initialize.TENSOR_DTYPES[k])]\n",
    "        values = values.groupby(level=0).apply(to_values)\n",
    "        tensor = tf.ragged.constant(values)\n",
    "        return tensor\n",
    "    \n",
    "    tensors['diagnosis'] = to_ragged_tensor(diagnosis, 'diagnosis')\n",
    "    \n",
    "    for k in ['height', 'weight', 'age']:\n",
    "        tensors[k] = to_ragged_tensor(metadata[k], k)\n",
    "    \n",
    "    k = 'chunk_count'\n",
    "    tensors[k] = to_nested_ragged_tensor(metadata[k], k)\n",
    "    \n",
    "    S = H['input_sigs'] + H['output_sigs']\n",
    "    for k in ['sig_index', 'adc_gain', 'baseline']:\n",
    "        tensors[k] = to_nested_ragged_tensor(sig_data[k][S], k)\n",
    "    \n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 672 ms, total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tensors = get_tensors(H, metadata, sig_data, diagnosis)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rec_ids': <tf.RaggedTensor [[3158849, 3933362]]>,\n",
       " 'diagnosis': <tf.RaggedTensor [[[-1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1], [-1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1]]]>,\n",
       " 'height': <tf.RaggedTensor [[-1, -1]]>,\n",
       " 'weight': <tf.RaggedTensor [[nan, nan]]>,\n",
       " 'age': <tf.RaggedTensor [[84, 84]]>,\n",
       " 'chunk_count': <tf.RaggedTensor [[16, 16, 16, 16], [16, 2, 4, 1, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]]>,\n",
       " 'sig_index': <tf.RaggedTensor [[[0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0]], [[0, 3, 0, 5, 6, 0, 2, 1, 4, 0, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 0, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 0, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 0, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 0, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 0, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 0, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0], [0, 3, 0, 5, 6, 0, 2, 1, 4, 7, 0, 0]]]>,\n",
       " 'adc_gain': <tf.RaggedTensor [[[0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 39.349998474121094, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 39.349998474121094, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 39.349998474121094, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 25.566699981689453, 0.0, 0.0]], [[0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 0.0, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 25.566699981689453, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 25.566699981689453, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 78.69999694824219, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 0.0, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 0.0, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 0.0, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 0.0, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 0.0, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 0.0, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 39.349998474121094, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 39.349998474121094, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 39.349998474121094, 0.0, 0.0], [0.0, 510.0, 0.0, 510.0, 512.0, 0.0, 1023.0, 1023.0, 4.261109828948975, 39.349998474121094, 0.0, 0.0]]]>,\n",
       " 'baseline': <tf.RaggedTensor [[[0, 256, 0, 256, 255, 0, 0, 0, 76, 274, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 274, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 274, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 76, 0, 0]], [[0, 256, 0, 256, 255, 0, 0, 0, 76, 0, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 76, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 76, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 471, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 0, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 0, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 0, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 0, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 0, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 0, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 274, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 274, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 274, 0, 0], [0, 256, 0, 256, 255, 0, 0, 0, 76, 274, 0, 0]]]>}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset.shuffle(tensors['rec_ids'].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1792"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = metadata[(sig_data['sig_index'][H['input_sigs_validation']] > 0).all(axis=1)].reset_index()['subject_id']\n",
    "s[s > 0].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5064"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = metadata.reset_index()['subject_id']\n",
    "s[s > 0].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4095"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = metadata[(sig_data['sig_index'][H['input_sigs_validation']] > 0).all(axis=1)].reset_index()['subject_id']\n",
    "s[s > 0].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4210"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = metadata[(sig_data['sig_index'][H['input_sigs_validation']] > 0).all(axis=1)].reset_index()['subject_id']\n",
    "s[s > 0].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7068"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = metadata[(sig_data['sig_index'][['II', 'PLETH']] > 0).all(axis=1)].reset_index()['subject_id']\n",
    "s[s > 0].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10067"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diagnosis.index.get_level_values(0).unique() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10099"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = metadata.reset_index()['subject_id']\n",
    "s[s > 0].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducers = {\n",
    "    'height': lambda i: i.max(),\n",
    "    'weight': lambda i: i.mean(),\n",
    "    'ethnicity': lambda i: i.mode()\n",
    "}\n",
    "\n",
    "with_hadm = matched_data.loc[(slice(None), slice(0, None)), :]\n",
    "reduced = {k: with_hadm[k].groupby(level=0).apply(reducers[k]) for k in reducers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in reduced:\n",
    "    matched_data.at[(slice(None), -1), k] = reduced[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hadm_id   -1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.Series({'hadm_id': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'height'\n",
    "with_hadm = matched_data.loc[(slice(None), slice(0, None)), k]\n",
    "reduced.at[:, k] = with_hadm.groupby(level=0).apply(reducers[k])\n",
    "reduced = pandas.DataFrame(reduced)\n",
    "reduced.at[:, 'hadm_id'] = -1\n",
    "reduced = reduced.reset_index().set_index(['subject_id', 'hadm_id'])\n",
    "reduced.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5433,)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_data.index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diagnosis_old(codes, metadata):\n",
    "    diagnosis = pandas.read_csv(clinic_file('diagnoses_icd'))\n",
    "    new_names = {'SUBJECT_ID': 'subject_id', 'HADM_ID': 'hadm_id'}\n",
    "    new_names['ICD9_CODE'] = 'code'\n",
    "    diagnosis = diagnosis[new_names].rename(columns=new_names)\n",
    "    diagnosis.loc[~diagnosis['code'].isin(codes), 'code'] = 'other'\n",
    "    diagnosis.drop_duplicates(inplace=True)\n",
    "    diagnosis.set_index(['subject_id', 'hadm_id', 'code'], inplace=True)\n",
    "    diagnosis.sort_index(inplace=True)\n",
    "    diagnosis.at[:, 'present'] = True\n",
    "    diagnosis = diagnosis.unstack(fill_value=False)['present'].astype('bool')\n",
    "    \n",
    "    index = metadata.reset_index()[diagnosis.index.names]\n",
    "    index = pandas.MultiIndex.from_frame(index)\n",
    "    diagnosis = diagnosis.reindex(index).reset_index()\n",
    "    frames = [diagnosis, metadata.reset_index()[['rec_id', 'segment']]]\n",
    "    diagnosis = pandas.concat(frames, sort=False, axis=1)\n",
    "\n",
    "    diagnosis = diagnosis.set_index(['subject_id', 'rec_id', 'segment'])\n",
    "    diagnosis = diagnosis.sort_index()\n",
    "    \n",
    "    is_negative_always = ~diagnosis.drop(columns='hadm_id').any(level=0)\n",
    "    is_diagnosed_always = (diagnosis['hadm_id'] > 0).all(level=0)\n",
    "    \n",
    "    is_negative = is_negative_always[is_diagnosed_always]\n",
    "    diagnosis = diagnosis.drop(columns='hadm_id')\n",
    "    bool_to_int = {True: 1, False: 0, numpy.nan: 0}\n",
    "    diagnosis = diagnosis.replace(bool_to_int)\n",
    "    diagnosis.loc[is_negative.index] -= is_negative.replace(bool_to_int)\n",
    "    diagnosis = diagnosis.reset_index()\n",
    "    diagnosis = diagnosis.set_index(metadata.index.names)\n",
    "    diagnosis = diagnosis.reindex(metadata.index)\n",
    "    return diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.32 s, sys: 2.9 s, total: 12.2 s\n",
      "Wall time: 20.6 s\n",
      "CPU times: user 45.6 s, sys: 12.7 s, total: 58.3 s\n",
      "Wall time: 58.3 s\n",
      "CPU times: user 57.4 s, sys: 15.8 s, total: 1min 13s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%time metadata, subject_ids = load_metadata()\n",
    "%time sig_data = load_sig_data(H['input_sigs'] + H['output_sigs'], subject_ids)\n",
    "%time diagnosis = load_diagnosis(H['icd_codes'])\n",
    "index = (metadata.index & sig_data.index).sort_values()\n",
    "metadata = metadata.reindex(index)\n",
    "sig_data = sig_data.reindex(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(H, metadata, sig_data):\n",
    "    assert((metadata.index == sig_data.index).all())\n",
    "    \n",
    "    tensors = {}\n",
    "    \n",
    "    get_rec_ids = lambda i: [i.index.get_level_values(1).unique()]\n",
    "    rec_ids = metadata.groupby(level=0).apply(get_rec_ids)\n",
    "    tensors['rec_ids'] = tf.ragged.constant(rec_ids, dtype='int32')\n",
    "    \n",
    "    row_lengths = metadata.groupby(level=[0, 1]).apply(len)\n",
    "    row_lengths = row_lengths.groupby(level=0).apply(lambda i: i.values)\n",
    "    row_lengths = [\n",
    "        row_lengths.apply(len).values,\n",
    "        numpy.concatenate(row_lengths.values)\n",
    "    ]\n",
    "    \n",
    "    segments = metadata.reset_index()['segment'].values.astype('int32')\n",
    "    segments = tf.RaggedTensor.from_nested_row_lengths(segments, row_lengths)\n",
    "    tensors['segments'] = segments\n",
    "    \n",
    "    S = H['input_sigs'] + H['output_sigs']\n",
    "    \n",
    "    for k in ['sig_index', 'adc_gain', 'baseline']:\n",
    "        tensors[k] = tf.RaggedTensor.from_nested_row_lengths(\n",
    "            sig_data[k][S].values, \n",
    "            row_lengths\n",
    "        )\n",
    "    \n",
    "    \n",
    "    gender = metadata.reset_index()[['subject_id', 'gender']]\n",
    "    gender = gender.drop_duplicates()['gender']\n",
    "    gender = gender.astype(object).replace({'M': 1, 'F': -1})\n",
    "    tensors['gender'] = gender.fillna(0).astype('int8')\n",
    "    \n",
    "    reducers = {\n",
    "        'height': lambda i: i.max(),\n",
    "        'weight': lambda i: i.mean(),\n",
    "#         'ethnicity': lambda i: i.mode()\n",
    "    }\n",
    "        \n",
    "    for k in reducers:\n",
    "        values = metadata.reset_index()[['subject_id', k]]\n",
    "        values = values.groupby('subject_id').apply(reducers[k])\n",
    "        tensors[k] = values[k]\n",
    "    \n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.7 s, sys: 136 ms, total: 38.8 s\n",
      "Wall time: 38.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rec_ids': <tf.RaggedTensor [[3563275, 3783672]]>,\n",
       " 'segments': <tf.RaggedTensor [[1, 2, 3, 6, 10], [9, 10, 13, 16, 17]]>,\n",
       " 'sig_index': <tf.RaggedTensor [[[0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0]], [[0, 1, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0], [0, 1, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0]]]>,\n",
       " 'adc_gain': <tf.RaggedTensor [[[0.0, 40.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0], [0.0, 39.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0], [0.0, 39.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0], [0.0, 41.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0], [0.0, 44.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0]], [[0.0, 50.0, 0.0, 120.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0], [0.0, 50.0, 0.0, 120.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0], [0.0, 57.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0], [0.0, 47.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0], [0.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0]]]>,\n",
       " 'baseline': <tf.RaggedTensor [[[0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0]]]>,\n",
       " 'gender': <tf.Tensor: id=1637, shape=(), dtype=int32, numpy=-1>,\n",
       " 'height': <tf.Tensor: id=1638, shape=(), dtype=int32, numpy=-1>,\n",
       " 'weight': <tf.Tensor: id=1642, shape=(), dtype=float32, numpy=nan>}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# tensors = get_tensors(H, metadata.loc[0:200], sig_data.loc[0:200])\n",
    "tensors = get_tensors(H, metadata, sig_data)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tensors)\n",
    "next(iter(dataset.shuffle(tensors['rec_ids'].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3544749 \t [5]\n",
      "3842928 3887555  [5] [9, 20]\n",
      "3860035 \t [29, 31]\n",
      "3485814 \t [7]\n",
      "3255538 \t [3, 4]\n"
     ]
    }
   ],
   "source": [
    "for i, j in dataset.take(5):\n",
    "    tabs = '\\t'*(2 - i[0].shape[0])\n",
    "    print(*i.to_list()[0], tabs, *j.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data = metadata.reset_index()\n",
    "matched_data = matched_data[matched_data['subject_id'] > 0]    \n",
    "matched_data.drop_duplicates(['subject_id', 'hadm_id'], inplace=True)\n",
    "matched_data.set_index(['subject_id', 'hadm_id'], inplace=True, verify_integrity=True)\n",
    "matched_data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     4869\n",
       "1       87\n",
       "2       35\n",
       "3       24\n",
       "4       21\n",
       "6        6\n",
       "5        5\n",
       "10       3\n",
       "8        3\n",
       "11       2\n",
       "7        2\n",
       "13       2\n",
       "9        2\n",
       "12       2\n",
       "14       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_data.groupby(level=0).apply(lambda i: i['age'].max() - i['age'].min()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.19999694824219"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_diffs = matched_data.groupby(level=0).apply(lambda i: i['weight'].max() - i['weight'].min())\n",
    "weight_diffs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height_diffs = matched_data.groupby(level=0).apply(lambda i: i['height'].max() - i['height'][i['height'] > 0].min())\n",
    "height_diffs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     5036\n",
       "1       13\n",
       "3        4\n",
       "2        4\n",
       "6        2\n",
       "4        2\n",
       "18       1\n",
       "5        1\n",
       "8        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height_diffs.fillna(0).astype('int32').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     4950\n",
       "3       15\n",
       "1       13\n",
       "6       12\n",
       "7       10\n",
       "2        9\n",
       "4        8\n",
       "5        5\n",
       "8        5\n",
       "11       4\n",
       "13       4\n",
       "9        4\n",
       "22       3\n",
       "12       3\n",
       "10       2\n",
       "15       2\n",
       "27       2\n",
       "14       2\n",
       "20       1\n",
       "32       1\n",
       "40       1\n",
       "31       1\n",
       "17       1\n",
       "25       1\n",
       "29       1\n",
       "18       1\n",
       "30       1\n",
       "19       1\n",
       "37       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_diffs.fillna(0).astype('int32').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5064"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_diffs.fillna(0).astype('int32').value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3865\n",
       "0    1199\n",
       "dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_diffs = matched_data.groupby(level=0).apply(lambda i: i['ethnicity'].nunique())\n",
    "race_diffs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
